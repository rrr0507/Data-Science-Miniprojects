{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import grader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapreduce\n",
    "\n",
    "## Introduciton\n",
    "\n",
    "We are going to be running mapreduce jobs on the wikipedia dataset.  The dataset is available (pre-chunked) on S3: `s3://dataincubator-course/mrdata/simple/`.  It may be downloaded with the `aws s3 sync` command or via HTTPS from `https://s3.amazonaws.com/dataincubator-course/mrdata/simple/part-000*`.\n",
    "\n",
    "For development, you can even use a single chunk (eg. part-00026.xml.bz2). That is small enough that mrjob can process the chunk in a few seconds. Your development cycle should be:\n",
    "\n",
    "1.  Get your job to work locally on one chunk.  This will greatly speed up your\n",
    "development.  To run on local:\n",
    "```bash\n",
    "python job_file.py -r local data/wikipedia/simple/part-00026.xml.bz2 > /tmp/output.txt\n",
    "```\n",
    "    \n",
    "2.  Get your job to work on the full dataset on GCP (Google Cloud Platform).  This will greatly speed up your production.  To run on GCP ([details](https://pythonhosted.org/mrjob/guides/dataproc-quickstart.html)):\n",
    "```bash\n",
    "python job_file.py -r dataproc data/wikipedia/simple/part-00026.xml.bz2 \\\n",
    "    --output-dir=gs://my-bucket/output/ \\\n",
    "    --no-output \n",
    "```\n",
    "\n",
    "    Not that you can also pass an entire local directory of data (eg. `data/simple/`) as the input.\n",
    "\n",
    "### Note on Memory\n",
    "There's a large difference between developing locally on one chunk and running your job on the entire dataset.  While you can get away with sloppy memory use locally, you really need to keep memory usage down if you hope to be able to complete the miniproject.  Remember, memory needs to be $O(1)$, not $O(n)$ in input.\n",
    "\n",
    "### Multiple Mapreduces\n",
    "You can combine multiple steps by overriding the [steps method](https://pythonhosted.org/mrjob/guides/writing-mrjobs.html#multi-step-jobs).  Usually your mapreduce might look like this\n",
    "```python\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class SingleMRJob(MRJob):\n",
    "    def mapper(self, key, value):\n",
    "        pass\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        pass\n",
    "```\n",
    "\n",
    "`MRJob` automatically uses the `mapper` and `reducer` methods.  To specify multiple steps, you need to override the `steps` method:\n",
    "\n",
    "```python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MultipleMRJob(MRJob):\n",
    "    def mapper1(self, key, value):\n",
    "        pass\n",
    "\n",
    "    def reducer1(self, key, values):\n",
    "        pass\n",
    "        \n",
    "    def mapper2(self, key, value):\n",
    "        pass\n",
    "\n",
    "    def reducer2(self, key, values):\n",
    "        pass\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper1, reducer=self.reducer1),\n",
    "            MRStep(mapper=self.mapper2, reducer=self.reducer2),\n",
    "        ]\n",
    "```\n",
    "\n",
    "As a matter of good style, we recommend that you actually write each individual mapreduce as it's own class.  Then write a wrapper module whose sole job is to combine those mapreduces by overriding `steps`.\n",
    "\n",
    "Some simple boilerplate for this, taking advantage of the default `steps` function that we get for free in a single-step MRJob class:\n",
    "\n",
    "```python\n",
    "class FirstStep(MRJob):\n",
    "  def mapper(self, key, value):\n",
    "    pass\n",
    "  def reducer(self, key, values):\n",
    "    pass\n",
    "  \n",
    "class SecondStep(MRJob):\n",
    "  def mapper(self, key, value):\n",
    "    pass\n",
    "  def reducer(self, key, values):\n",
    "    pass\n",
    "  \n",
    "class SteppedJob(MRJob):\n",
    "  \"\"\"\n",
    "  A two-step job that first runs FirstStep's MR and then SecondStep's MR\n",
    "  \"\"\"\n",
    "  def steps(self):\n",
    "    return FirstStep().steps() + SecondStep().steps()\n",
    "```\n",
    "\n",
    "\n",
    "### Note on Style\n",
    "Here are some helpful articles on how mrjob works and how to pass parameters to your script:\n",
    "  - [How mrjob is run](https://pythonhosted.org/mrjob/guides/concepts.html#how-your-program-is-run)\n",
    "  - [Adding passthrough options](https://pythonhosted.org/mrjob/job.html#mrjob.job.MRJob.add_passthrough_option)\n",
    "  - [An example of someone solving similar problems](http://arunxjacob.blogspot.com/2013/11/hadoop-streaming-with-mrjob.html)\n",
    "\n",
    "See the notebook \"Hadoop MapReduce with mrjob\" in the datacourse for more details.\n",
    "\n",
    "Finally, if you are find yourself processing a lot of special cases, you are probably doing it wrong.  For example, mapreduce jobs for `Top100WordsSimpleWikipediaPlain`, `Top100WordsSimpleWikipediaText`, and `Top100WordsSimpleWikipediaNoMetaData` are less than 150 lines of code (including generous blank lines and biolerplate code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: top100_words_simple_plain\n",
    "Return a list of the top 100 words in an article text (in no particular order). You will need to write this as two map reduces:\n",
    "\n",
    "1. The first job is similar to standard wordcount but with a few tweaks. The data provided for wikipedia is in `*.xml.bz2` format.  Mrjob will automatically decompress `bz2`.  We'll deal with the `xml` in the next question. For now, just treat it as text.  A few hints:\n",
    "   - To split the words, use the regular expression \"\\w+\".\n",
    "   - Words are not case sensitive: i.e. \"The\" and \"the\" reference to the same word.  You can use `string.lower()` to get a single case-insenstive canonical version of the data.\n",
    "\n",
    "2. The second job will take a collection of pairs `(word, count)` and filter for only the highest 100.  A few notes:\n",
    "    - **Passing parameters:** To make the job more reusable make the job find the largest `n` words where `n` is a parameter obtained via [`get_jobconf_value`](https://pythonhosted.org/mrjob/utils-compat.html).\n",
    "    - **Keeping track of the top n:** We have to keep track of at most the `n` most popular words.  As long as `n` is small, e.g. 100, we can keep track of the *running largest n* in memory wtih a priority-queue. We suggest taking a look at `heapq` ([details](https://docs.python.org/2/library/heapq.html)), part of the Python standard library for this.  It allows you to push elemnets into a list while keeping track of the highest priority element.\n",
    "```python\n",
    "h = []\n",
    "heappush(h, (5, 'write code'))\n",
    "heappush(h, (7, 'release product'))\n",
    "heappush(h, (1, 'write spec'))\n",
    "heappush(h, (3, 'create tests'))\n",
    "heappop(h)  // returns (1, 'write spec')\n",
    "```\n",
    "   \n",
    "       A naive implementation would cost $O(1)$ to insert but $O(n)$ to retrieve.  `heapq` uses a [self-balancing binary search tree](https://en.wikipedia.org/wiki/Self-balancing_binary_search_tree) to enable $O(\\log(n))$ insertion and $O(1)$ removal. You may be asked about this data structure on an interview so it is good to get practice with it now.\n",
    "    - **Working across nodes:** To obtain the largest `n`, we need to first obtain the largest n elements per chunk from the mapper, output them to the same key (reducer), and then collect the largest n elements of those in the reducer (**Question:** why does this gaurantee that we have found the largest n over the entire set?)\n",
    "    - **Working within a node:** Given that we are using a priority queue, we will need to first initialize it, then `push` or `pushpop` each record to it, and finally output the top `n` after seeing each record.  For mappers, notice that these three phases correspond nicely to these three functions:\n",
    "        - `mapper_init`\n",
    "        - `mapper`\n",
    "        - `mapper_final`\n",
    "\n",
    "    There are similar functions in the reducer.  Also, while the run method to launch the mapreduce job is a classmethod:\n",
    "        ```python\n",
    "          if __name__ == '__main__':\n",
    "            MRWordCount.run()\n",
    "        ```\n",
    "     actual instances of our mapreduce are instantiated on the map and reduce nodes.  More precisely, a separate mapper class is instantiated in each map node and a reducer class is instantiated in each reducer node.  This means that the three mapper functions can pass state through `self`, e.g. `self.heap`. Remember that to pass state between the map and reduce phase, you will have to use `yield` in the mapper and read each line in the reducer. (**Question:** Can you pass state between two mappers?)\n",
    "\n",
    "**Checkpoint:**\n",
    "- Total unique words: 1,584,646"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('output.txt','r')\n",
    "output = []\n",
    "for i in range(100):\n",
    "    line = f.readline().split()\n",
    "    word = line[0].strip('\"')\n",
    "    output.append((str(word),int(line[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  0.96\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "def top100_words_simple_plain():\n",
    "    return output\n",
    "\n",
    "grader.score(question_name='mr__top100_words_simple_plain', func=top100_words_simple_plain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: top100_words_simple_text\n",
    "Notice that the words \"page\" and \"text\" make it into the top 100 words in the previous problem.  These are not common English words!  If you look at the xml formatting, you'll realize that these are xml tags.  You should parse the files so that tags like `<page></page>` should not be included in your total, nor should words outside of the tag `<text></text>`.\n",
    "\n",
    "**Hints**:\n",
    "1. Both `xml.etree.elementtree` from the Python stdlib or `lxml.etree` parse xml. `lxml` is significantly faster though and avoids some bugs.\n",
    "\n",
    "2. In order to parse the text, we will have to accumulate a `<page></page>` worth of data and then split the resulting string into words.\n",
    "\n",
    "3. Don't forget that the Wikipedia format can have multiple revisions but you only want the latest one.\n",
    "\n",
    "4. What happens if a content from a page is split across two different mappers? How does this problem scale with data size?\n",
    "\n",
    "**Checkpoint:**\n",
    "- Total unique words: 867,871"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('output_2.txt','r')\n",
    "output_2 = []\n",
    "for i in range(100):\n",
    "    line = f.readline().split()\n",
    "    word = line[0].strip('\"')\n",
    "    output_2.append((word,int(line[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  0.95\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "def top100_words_simple_text():\n",
    "    return output_2\n",
    "\n",
    "grader.score(question_name='mr__top100_words_simple_text', func=top100_words_simple_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: top100_words_simple_no_metadata\n",
    "\n",
    "Finally, notice that 'www' and 'http' make it into the list of top 100 words in the previous problem.  These are also not common English words either!  These are clearly from the url in hyperlinks.  Looking at the format of [Wikipedia links](http://en.wikipedia.org/wiki/Help:Wiki_markup#Links_and_URLs) and [citations](http://en.wikipedia.org/wiki/Help:Wiki_markup#References_and_citing_sources), you'll notice that they tend to appear within single and double brackets and curly braces.\n",
    "\n",
    "**Hint**:\n",
    "You can either write a simple parser to eliminate the urls within brackets, angle braces, and curly braces or you can use a package like the colorfully-named [mwparserfromhell](https://github.com/earwig/mwparserfromhell/), which has been provisioned on `mrjob` and supports the convenient helper function `strip_code()` (which is used by the reference solution).\n",
    "\n",
    "**Checkpoint:**\n",
    "- Total unique words: 618,410"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('output_3.txt','r')\n",
    "output_3 = []\n",
    "for i in range(100):\n",
    "    line = f.readline().split()\n",
    "    word = line[0].strip('\"')\n",
    "    output_3.append((word,int(line[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  0.95\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "def top100_words_simple_no_metadata():\n",
    "    return output_3\n",
    "\n",
    "grader.score(question_name='mr__top100_words_simple_no_metadata', func=top100_words_simple_no_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: link_stats_simple\n",
    "Let's look at some summary statistics on the number of unique links on a page to other Wikipedia articles.  Return the number of articles (count), average number of links, standard deviation, and the 25%, median, and 75% quantiles.\n",
    "\n",
    "1. Notice that the library `mwparserfromhell` supports the method `filter_wikilinks()`.\n",
    "2. You will need to compute these statistics in a way that requires O(1) memory.  You should be able to compute the first few (i.e. non-quantile) statistics exactly by looking at the first few moments of a distribution. The quantile quantities can be accurately estimated by using reservoir sampling with a large reservoir.\n",
    "3. If there are multiple links to the article have it only count for 1.  This keeps our results from becoming too skewed.\n",
    "4. Don't forget that some (a surprisingly large number of) links have unicode! Make sure you treat them correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  1.0\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "def link_stats_simple():\n",
    "    return [\n",
    "        (\"count\", 188418),\n",
    "        (\"mean\",  15.5),\n",
    "        (\"stdev\", 50.5),\n",
    "        (\"25%\", 1),\n",
    "        (\"median\", 6),\n",
    "        (\"75%\", 16),\n",
    "    ]\n",
    "\n",
    "grader.score(question_name='mr__link_stats_simple', func=link_stats_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2016 The Data Incubator.  All rights reserved.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
